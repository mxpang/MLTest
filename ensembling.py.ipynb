{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state']=seed\n",
    "        self.clf=clf(**params)\n",
    "        \n",
    "    def train(self, xtrain, ytrain):\n",
    "        self.clf.fit(xtrain, ytrain)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self, x, y):\n",
    "        return self.clf.fit(x,y).feature_importances_\n",
    "    \n",
    "def train_multi_clf(clfs, xtrain, ytrain, xtest, n=5):\n",
    "    skf = StratifiedKFold(n_splits=n)\n",
    "    train_pred = np.zeros((xtrain.shape[0], len(clfs)))\n",
    "    test_pred = np.zeros((xtest.shape[0], len(clfs)))\n",
    "\n",
    "    for i, clf in enumerate(clfs):\n",
    "        #for each classifier, perform kfold validation\n",
    "        print('training using [%s]' % clf.__class__.__name__)\n",
    "        test_pred_i = np.zeros((xtest.shape[0], n))\n",
    "        for j, (train_index, cv_index) in enumerate(skf.split(xtrain, ytrain)):\n",
    "            print('Fold {0}, size 0f train is {1}, size of test is {2}'.format(j, len(train_index), len(cv_index)))\n",
    "            x_train, x_cv = xtrain1.iloc[train_index,:], xtrain1.iloc[cv_index, :]\n",
    "            y_train, y_cv = ytrain1.iloc[train_index], ytrain1.iloc[cv_index]\n",
    "        \n",
    "            clf.fit(x_train, y_train)\n",
    "        \n",
    "            train_pred[cv_index, i]=clf.predict(x_cv)[:,1]\n",
    "            test_pred_i[:, j]=clf.predict(xtest1)[:,1]\n",
    "        test_pred[:,i]=test_pred_i.mean(1)\n",
    "    return train_pred, test_pred  #numpy array\n",
    "\n",
    "def pred_test_using_multi_clfs(clf, train_pred_comb, test_pred_comb):\n",
    "    '''\n",
    "    input: predictions generated from the train_multi_clf.\n",
    "    combine predictions:\n",
    "    train_pred_comb=np.concatenate((et_train, rf_train), axis=1)\n",
    "    '''\n",
    "\n",
    "def feat_imp(clfs, xtrain, ytrain):\n",
    "    feat_imp_dict = {}\n",
    "    feat_imp_dict['features'] = xtrain.columns.values\n",
    "    for clf in clfs:\n",
    "        feat_imp_dict[clf.__class__.__name__] = clf.feature_importances(xtrain, ytrain)\n",
    "    feat_df = pd.DataFrame.from_dict(feat_imp_dict)\n",
    "    feat_df.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.show()\n",
    "    return feat_df\n",
    "        \n",
    "        \n",
    "        \n",
    "def plot_conf_roc(test_true, test_pred, pred_proba):\n",
    "    '''\n",
    "    - input: true value, predict value and predicted probabilities for roc-auc curve\n",
    "    '''\n",
    "    cnf_matrix = metrics.confusion_matrix(test_true, test_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True,\n",
    "                      title='Confusion matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n-----------roc curve-------------')\n",
    "    fpr, tpr, thresholds= metrics.roc_curve(test_true, pred_proba)\n",
    "    roc_auc=metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "if __name__=='main':\n",
    "    rf_params = {'n_jobs':-1, 'n_estimators':500, 'max_depth':6, 'max_features':'sqrt', 'verbose':0}\n",
    "    et_params = {'n_jobs':-1, 'n_estimators':500, 'max_depth': 8, 'min_sample_leaf': 2, 'verbose':0}\n",
    "    rf = SklearnHelper(clf=RandomForestClassifier, seed=0, params=rf_params)\n",
    "    et = SklearnHelper(clf=ExtraTreesClassifier, seed=0, params=et_params)\n",
    "    xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.3)\n",
    "    clf_train, clf_test = train_multi_clf([rf, et], xtrain, ytrain, xtest, n=5)\n",
    "    #et_train, et_test = train_multi_clf(et, xtrain, ytrain, xtest, n=5)\n",
    "    \n",
    "    #perform the second level learning model via XGBoost\n",
    "    gbm = xgb.XGBClassifier(\n",
    "        n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8,\n",
    "        colsample_bytree=0.8, objective='binary:logistic', nthread=-1, scale_pos_weight=1\n",
    "    ).fit(clf_train, ytrain)\n",
    "    predictions=gbm.predict(xtest)\n",
    "    '''\n",
    "    generate datframe to check distribution status\n",
    "    base_predictions_train=pd.DataFrame({'RandomForest': rf_train.ravel(),\n",
    "    'ExtraTrees': et_train.ravel(),\n",
    "    'AdaBoost':ada_train.ravel(),\n",
    "    'GradientBoost':gb_train.ravel()})\n",
    "    -----------------------------------------------\n",
    "    |AdaBoost|ExtraTrees|GradientBoost|RandomForest|\n",
    "    |0.0     |0.0       |0.0          |0.0\n",
    "    |1.0     |1.0       |1.0          |1.0\n",
    "    |1.0     |0.0       |1.0          |0.0\n",
    "    |1.0     |1.0       |1.0          |1.0\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
